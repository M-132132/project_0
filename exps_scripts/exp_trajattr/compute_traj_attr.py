"""
è½¨è¿¹é¢„æµ‹å½’å› è®¡ç®—ä¸»è„šæœ¬

ä½¿ç”¨æ–¹æ³•:
    python exps_scripts/exp_trajattr/compute_traj_attr.py --config-name autobot_attr
    python exps_scripts/exp_trajattr/compute_traj_attr.py --config-name wayformer_attr model_name=wayformer
"""

import os
import sys
import json
import time
from datetime import datetime
from pathlib import Path
import numpy as np
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
import hydra
from omegaconf import DictConfig, OmegaConf

from models import build_model
from utils_datasets_traj import build_dataset
from utils.utils_train_traj import set_seed

from utils.path_manager import path_manager

# å¯¼å…¥å½’å› è®¡ç®—æ¡†æ¶
from utils_attr.traj_attr.base.traj_attr_base import TrajAttrBase


@hydra.main(version_base=None, config_path=str(path_manager.get_config_path()),
            config_name="traj_attr_base")
def main(cfg: DictConfig) -> None:
    # å¯ç”¨é…ç½®ä¿®æ”¹
    OmegaConf.set_struct(cfg, False)
    cfg = OmegaConf.merge(cfg, cfg.method)
    cfg = OmegaConf.merge(cfg, cfg.attribution)
    
    # åˆ›å»ºå®éªŒå¯¹è±¡
    experiment = TrajAttrExperiment(cfg)
    
    # è¿è¡Œå®éªŒ
    attribution_results, analysis_results = experiment.run_attribution_experiment()
    
    # æ‰“å°æˆåŠŸä¿¡æ¯
    print(f"\nâœ“ å®éªŒæˆåŠŸå®Œæˆï¼")
    print(f"âœ“ ç»“æœä¿å­˜åœ¨: {experiment.paths['base']}")

    # å¦‚æœå¯ç”¨å¯è§†åŒ–ï¼Œæç¤ºè¿è¡Œå¯è§†åŒ–è„šæœ¬
    if cfg.visualization.enable:
        print(f"\nğŸ’¡ è¿è¡Œå¯è§†åŒ–è„šæœ¬:")
        print(f"python exps_scripts/exp_trajattr/visualize_traj_attr.py "
              f"result_path={experiment.paths['base']}")
        
        
class TrajAttrExperiment:
    """è½¨è¿¹é¢„æµ‹å½’å› è®¡ç®—å®éªŒç±»"""
    
    def __init__(self, config: DictConfig):
        self.config = config
        self.device = torch.device('cuda' if torch.cuda.is_available() and not config.debug else 'cpu')
        
        # è®¾ç½®éšæœºç§å­
        set_seed(config.seed)
        
        # è®¾ç½®ä¿å­˜è·¯å¾„
        self.setup_save_paths()
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.model = None
        self.val_loader = None
        self.attributor = None
        
    def setup_save_paths(self):
        """è®¾ç½®ä¿å­˜è·¯å¾„"""
        # ä½¿ç”¨ path_manager è§£æåŸºç¡€è·¯å¾„
        base_dir = path_manager.resolve_path(self.config.save_config.base_dir)
        model_name = self.config.model_name
        dataset_name = self.config.dataset_name
        
        # åˆ›å»ºæ—¶é—´æˆ³ç›®å½•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.save_config.create_timestamp_dir:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            exp_dir = f"{base_dir}/{model_name}_{dataset_name}/{timestamp}"
        else:
            exp_dir = f"{base_dir}/{model_name}_{dataset_name}"
            
        # åˆ›å»ºå­ç›®å½•
        self.paths = {
            'base': Path(exp_dir),
            'attributions': Path(exp_dir) / 'attributions',
            'numpy': Path(exp_dir) / 'attributions' / 'numpy',
            'heatmaps': Path(exp_dir) / 'attributions' / 'heatmaps',
            'statistics': Path(exp_dir) / 'attributions' / 'statistics',
            'visualizations': Path(exp_dir) / 'visualizations',
            'trajectory_plots': Path(exp_dir) / 'visualizations' / 'trajectory_plots',
            'map_plots': Path(exp_dir) / 'visualizations' / 'map_plots',
            'importance_analysis': Path(exp_dir) / 'visualizations' / 'importance_analysis',
            'reports': Path(exp_dir) / 'reports',
            'configs': Path(exp_dir) / 'configs'
        }
        
        # åˆ›å»ºæ‰€æœ‰ç›®å½•
        for path in self.paths.values():
            path.mkdir(parents=True, exist_ok=True)
            
        print(f"å®éªŒç»“æœå°†ä¿å­˜åˆ°: {self.paths['base']}")
        
    def load_model_and_data(self):
        """åŠ è½½æ¨¡å‹å’Œæ•°æ®"""
        print(f"åŠ è½½ {self.config.model_name} æ¨¡å‹...")
        
        # æ„å»ºæ¨¡å‹
        self.model = build_model(self.config).to(self.device)
        
        # åŠ è½½æ£€æŸ¥ç‚¹
        if self.config.ckpt_path and Path(path_manager.resolve_path(self.config.ckpt_path)).exists():
            weight_path = str(path_manager.resolve_path(self.config.ckpt_path))
            print(f"ä»æ£€æŸ¥ç‚¹åŠ è½½: {weight_path}")
            checkpoint = torch.load(weight_path, map_location=self.device, weights_only=False)
            
            if 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
            elif 'state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['state_dict'])
            else:
                self.model.load_state_dict(checkpoint)
                
            print("æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸ")
        else:
            print(f"è­¦å‘Š: æ£€æŸ¥ç‚¹è·¯å¾„ä¸å­˜åœ¨ {self.config.ckpt_path}ï¼Œä½¿ç”¨éšæœºæƒé‡")
            
        # æ„å»ºæ•°æ®é›†
        print("åŠ è½½éªŒè¯æ•°æ®é›†...")
        val_dataset = build_dataset(self.config, val=True)
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.config.method.get('eval_batch_size', 2),
            num_workers=self.config.load_num_workers,
            shuffle=False,
            drop_last=False,
            collate_fn=val_dataset.collate_fn,
            pin_memory=torch.cuda.is_available()
        )
        
        print(f"æ•°æ®é›†åŠ è½½å®Œæˆ: {len(val_dataset)} ä¸ªæ ·æœ¬, {len(self.val_loader)} ä¸ªæ‰¹æ¬¡")
        
    def create_attributor(self):
        """åˆ›å»ºå½’å› è®¡ç®—å™¨"""
        model_name = self.config.model_name.lower()
        
        # ç›´æ¥ä½¿ç”¨å®Œæ•´çš„DictConfigï¼Œå¹¶ä¼ å…¥ä¿å­˜è·¯å¾„
        # DictConfig å·²ç»åŒ…å«äº† dirichlet_config, guided_ig_config, captum_config ç­‰
        self.attributor = TrajAttrBase(self.model, self.config, self.paths)
            
        print(f"åˆ›å»ºäº† {model_name} å½’å› è®¡ç®—å™¨ï¼ˆä½¿ç”¨ç»Ÿä¸€é€‚é…å™¨ï¼‰")
        
        
    def run_attribution_experiment(self):
        """è¿è¡Œå®Œæ•´çš„å½’å› å®éªŒ"""
        print("="*60)
        print(f"å¼€å§‹è½¨è¿¹é¢„æµ‹å½’å› å®éªŒ: {self.config.exp_name}")
        print(f"æ¨¡å‹: {self.config.model_name}")
        print(f"æ•°æ®é›†: {self.config.dataset_name}")
        print(f"å½’å› æ–¹æ³•: {self.config.attribution.methods}")
        print("="*60)
        
        # ä¿å­˜å®éªŒé…ç½®
        self.save_experiment_config()
        
        # åŠ è½½æ¨¡å‹å’Œæ•°æ®
        self.load_model_and_data()
        
        # åˆ›å»ºå½’å› è®¡ç®—å™¨
        self.create_attributor()
        
        # è®¾ç½®æ¨¡å‹ä¸ºè¯„ä¼°æ¨¡å¼
        self.model.eval()
        
        # è¿è¡Œå½’å› è®¡ç®—
        attribution_results = self.compute_batch_attributions()
        
        # åˆ†æç»“æœ
        if self.config.analysis.generate_summary_statistics:
            analysis_results = self.analyze_attribution_results(attribution_results)
        else:
            analysis_results = {}
            
        # ç”Ÿæˆå®éªŒæŠ¥å‘Š
        self.generate_experiment_report(attribution_results, analysis_results)
        
        print("="*60)
        print("å½’å› å®éªŒå®Œæˆï¼")
        print(f"ç»“æœä¿å­˜åœ¨: {self.paths['base']}")
        print("="*60)
        
        return attribution_results, analysis_results
        
    def compute_batch_attributions(self):
        """æ‰¹é‡è®¡ç®—å½’å› """
        print(f"å¼€å§‹è®¡ç®—å½’å› ï¼Œé™åˆ¶æ‰¹æ¬¡æ•°: {self.config.attribution.batch_limit}")
        
        attribution_results = []
        batch_limit = self.config.attribution.batch_limit
        
        for batch_idx, batch in enumerate(tqdm(self.val_loader, desc='å½’å› ä¸­')):
            if batch_idx >= batch_limit:
                break
            
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            self._move_to_device(batch, self.device)
            # è®¡ç®—å½’å› 
            # å¯ç”¨æ¢¯åº¦è®¡ç®—
            torch.set_grad_enabled(True)
            
            # åˆ›å»ºå…ƒæ•°æ®
            metadata = {
                'batch_idx': batch_idx,
                'model_name': self.config.model_name,
                'batch_size': self._get_batch_size(batch),
                'timestamp': datetime.now().isoformat()
            }
            
            # è®¡ç®—å½’å› 
            batch_attributions = self.attributor.compute_and_save_attribution(
                batch,
                methods=self.config.attribution.methods,
                metadata=metadata
            )
            
            # è®¡ç®—é¢å¤–åˆ†æï¼ˆå¦‚æœæ˜¯æ¨¡å‹ç‰¹å®šå½’å› å™¨ï¼‰
            analysis = {}
            if hasattr(self.attributor, 'compute_feature_importance'):
                for method, attrs in batch_attributions.items():
                    try:
                        importance = self.attributor.compute_feature_importance(attrs, batch)
                        analysis[f'{method}_importance'] = importance
                    except Exception as e:
                        print(f"è®¡ç®— {method} ç‰¹å¾é‡è¦æ€§æ—¶å‡ºé”™: {e}")
            
            # ä¿å­˜æ‰¹æ¬¡ç»“æœ
            batch_result = {
                'metadata': metadata,
                'attributions': batch_attributions,
                'analysis': analysis
            }
            
            attribution_results.append(batch_result)
        
        print(f"å½’å› è®¡ç®—å®Œæˆï¼ŒæˆåŠŸå¤„ç† {len(attribution_results)} ä¸ªæ‰¹æ¬¡")
        return attribution_results
        
    def analyze_attribution_results(self, attribution_results):
        """åˆ†æå½’å› ç»“æœ"""
        print("åˆ†æå½’å› ç»“æœ...")
        
        if not attribution_results:
            return {}
            
        analysis = {
            'summary': {
                'total_batches': len(attribution_results),
                'methods': self.config.attribution.methods,
                'model_name': self.config.model_name,
                'total_samples': sum(r['metadata']['batch_size'] for r in attribution_results)
            },
            'method_statistics': {},
            'importance_statistics': {}
        }
        
        # ç»Ÿè®¡æ¯ç§æ–¹æ³•çš„æˆåŠŸç‡
        for method in self.config.attribution.methods:
            success_count = sum(1 for r in attribution_results if method in r['attributions'])
            analysis['method_statistics'][method] = {
                'success_rate': success_count / len(attribution_results),
                'success_count': success_count,
                'total_count': len(attribution_results)
            }
        
        # ç»Ÿè®¡é‡è¦æ€§åˆ†æ
        if attribution_results[0]['analysis']:
            importance_keys = list(attribution_results[0]['analysis'].keys())
            for key in importance_keys:
                analysis['importance_statistics'][key] = {
                    'available_batches': sum(1 for r in attribution_results if key in r['analysis'])
                }
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_path = self.paths['reports'] / 'attribution_analysis.json'
        with open(analysis_path, 'w', encoding='utf-8') as f:
            json.dump(self._convert_numpy_types(analysis), f, indent=2, ensure_ascii=False)
            
        return analysis
        
    def save_experiment_config(self):
        """ä¿å­˜å®éªŒé…ç½®"""
        config_path = self.paths['configs'] / 'experiment_config.yaml'
        with open(config_path, 'w', encoding='utf-8') as f:
            OmegaConf.save(config=self.config, f=f)
            
    def generate_experiment_report(self, attribution_results, analysis_results):
        """ç”Ÿæˆå®éªŒæŠ¥å‘Š"""
        report = {
            'experiment_info': {
                'exp_name': self.config.exp_name,
                'model_name': self.config.model_name,
                'dataset_name': self.config.dataset_name,
                'start_time': datetime.now().isoformat(),
                'config_file': str(self.paths['configs'] / 'experiment_config.yaml')
            },
            'attribution_settings': {
                'methods': self.config.attribution.methods,
                'batch_limit': self.config.attribution.batch_limit,
                'distance_type': self.config.attribution.distance_type
            },
            'results_summary': analysis_results.get('summary', {}),
            'method_performance': analysis_results.get('method_statistics', {}),
            'paths': {k: str(v) for k, v in self.paths.items()}
        }
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = self.paths['reports'] / 'experiment_report.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
            
        # ç”Ÿæˆç®€è¦æŠ¥å‘Š
        self._print_summary_report(report)
        
    def _print_summary_report(self, report):
        """æ‰“å°ç®€è¦æŠ¥å‘Š"""
        print("\n" + "="*50)
        print("å®éªŒæ€»ç»“æŠ¥å‘Š")
        print("="*50)
        print(f"å®éªŒåç§°: {report['experiment_info']['exp_name']}")
        print(f"æ¨¡å‹: {report['experiment_info']['model_name']}")
        print(f"æ•°æ®é›†: {report['experiment_info']['dataset_name']}")
        
        if 'results_summary' in report and report['results_summary']:
            summary = report['results_summary']
            print(f"å¤„ç†æ‰¹æ¬¡: {summary.get('total_batches', 0)}")
            print(f"æ€»æ ·æœ¬æ•°: {summary.get('total_samples', 0)}")
            
        print("\nå½’å› æ–¹æ³•æ€§èƒ½:")
        for method, stats in report.get('method_performance', {}).items():
            print(f"  {method}: {stats['success_count']}/{stats['total_count']} "
                  f"({stats['success_rate']:.2%} æˆåŠŸç‡)")
                  
        print(f"\nç»“æœä¿å­˜è·¯å¾„: {report['paths']['base']}")
        print("="*50)
        
    def _move_to_device(self, batch, device):
        """é€’å½’åœ°å°†batchæ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡"""
        if isinstance(batch, dict):
            for key, value in batch.items():
                if isinstance(value, torch.Tensor):
                    batch[key] = value.to(device)
                elif isinstance(value, dict):
                    self._move_to_device(value, device)
                elif isinstance(value, list) and len(value) > 0 and isinstance(value[0], torch.Tensor):
                    batch[key] = [v.to(device) for v in value]
                    
    def _get_batch_size(self, batch):
        """è·å–æ‰¹æ¬¡å¤§å°"""
        if 'input_dict' in batch:
            for key, value in batch['input_dict'].items():
                if isinstance(value, torch.Tensor):
                    return value.size(0)
        
        for key, value in batch.items():
            if isinstance(value, torch.Tensor):
                return value.size(0)
        
        return 1
        
    def _convert_numpy_types(self, obj):
        """è½¬æ¢numpyç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
        if isinstance(obj, dict):
            return {k: self._convert_numpy_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_numpy_types(item) for item in obj]
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, (np.integer, np.floating)):
            return obj.item()
        elif isinstance(obj, torch.Tensor):
            return obj.detach().cpu().numpy().tolist()
        else:
            return obj


if __name__ == "__main__":
    main()
    